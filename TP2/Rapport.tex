\documentclass{report}
\usepackage{MCC}

\def\footauthor{Thomas COUCHOUD \& Victor COLEAU}
\title{Java Performance - TP2}
\author{Thomas COUCHOUD\\\texttt{thomas.couchoud@etu.univ-tours.fr}\\Victor COLEAU\\\texttt{victor.coleau@etu.univ-tours.fr}}

\rowcolors{1}{white}{white}
\begin{document}
	\mccTitle
	
	\chapter{Etude de JMH}
		\section{Annotations}
			JMH fonctionne principalement avec des annotations.
			Parmi celles-ci nous retrouvons notamment:
			\begin{easylist}[itemize]
				@ \textbf{Benchmark}: Permet de définir une méthode comme étant une méthode à tester
				@ \textbf{BenchmarkMode}: Permet d'indiquer quelles métriques nous voulons étudier, nous avons:
				@@ Throughput: Mesure le nombre d'opérations par seconde
				@@ Average Time: Mesure le temps moyen d'exécution de la méthode
				@@ Sample Time: Mesure les temps d'exécution de la méthode: min, max, ...
				@@ Single Shot Time: Mesure le temps d'exécution pour un run unique du benchmark.
				Cela peut être utile pour voir la performance de la méthode sans le hotspot.
				@ \textbf{OutputTimeUnit}: Permet de définir les unités de temps utilisées pour les rapports
				@ \textbf{Fork}: Permet de définir le nombre de JVM qui sont forkées.
				En effet par défaut la JVM est forkée à chaque "trial" de la méthode à benchmarker, cependant dans certains cas il peut être utile de changer ce comportement.
				Il faut tout de même faire attention lors de l'utilisation de cette annotation car des comportements anormaux peuvent apparaître (si l'un a deux classes implémentant la même interface par exemple, en effet dans ce cas la première sera plus rapide car la JVM remplace les appels méthodes).
				@ \textbf{Group}: Cette annotation permet de définir à quel groupe appartient le test.
				Cela permet de lancer tous les tests d'un même groupe en même temps (voir annotation suivante).
				Par défaut chaque méthode est dans un groupe unique.
				@ \textbf{GroupThread}: Définit le nombre de threads qui sera utilisé pour lancer la méthode.
				Dans le cas d'un groupe il y aura un nombre de threads égal à la somme de tous les GroupThreads du groupe.
				@ \textbf{Measurement}: Permet de définir les valeurs par défaut pour le benchmark de la méthode:
				@@ Nombre d'itérations
				@@ Temps de mesure
				@@ Taille d'un batch
				@ \textbf{State}: Annotation à mettre sur une classe afin de définir dans quel état cette dernière sera utilisée:
				@@ Thread: Valeur par défaut. Une instance de cette classe sera utilisée pour chaque thread qui exécute le benchmark.
				@@ Benchmark: Une instance unique sera partagée entre tous les threads qui exécutent le même benchmark. 
				Peut être utile dans le cas de méthodes exécutées en parrallèle.
				@@ Group: Une instance sera allouée pour chaque groupe.
				@ \textbf{TearDown / Setup}: Cette annotation permet de marquer les méthodes à lancer avant et après les benchmark.
				L'appel à ces méthodes peuvent être précisés grâce a une paramètre de l'annotation:
				@@ Trial: Valeur par défaut. La méthode est appelée après/avant un benchmark entier.
				@@ Iteration: La méthode est appelée avant/après une itération (plusieurs invocations).
				@@ Invocation: La méthode est appelée avant/après une invocation (chaque run).
				@ \textbf{Timeout}: Définit le temps maximum que le benchmark peut prendre.
				@ \textbf{Warmup}: Définit les paramètres pour la phase de chauffe.
				Similaire à Measurement mais concerne juste la phase de chauffe.
				@ \textbf{CompilerControl}: Définit les options de compilation lors d'un fork de la JVM pour le benchmark de cette méthode.
				Cependant il faut faire attention car le compilateur peut ignorer ces derniers.
			\end{easylist}
		
		\section{BalckHole}
			Le BlackHole permet de consommer, sans donner d'informations si la valeur est réellement utilisée ou non à JIT (compilation à la volée), des variables ou bien du temps CPU.
			
			Dans le cas de variables cela est utile car cela évite que le compilateur réalise des optimisations où la variable ne serait pas calculée.
			
		\section{Résultats de MyBenchmark}
			(Code disponible en \autoref{sec:mybenchmark})
		
			Voici la sortie pour un benchmark:
			\lstinputlisting[caption=out.log]{out.log}
			
			On retrouve entre les lignes 1 à 10 les différentes informations concernant le test.
			Le reste du fichier (L12-75) contient tous les run.
			A la fin (L78-81) nous obtenons les résultats finaux.
			
			\subsection{Informations}
				On retrouve les informations concernant la JVM (version JDK, version VM, options, ...) ainsi que la version de JMH.
				De plus nous avons les paramètres de JMH concernant le nombre d'itérations et le temps pour les phases de chauffe et de test (voir les annotations Measurement et Warmup).
				Les valeurs des annotations Timeout, BenchmarkMode, Thread sont aussi présentes.
			
			\subsection{Runs}
				Pour chaque run on nous indique la progression totale du benchmark ainsi qu'une estimation du temps restant.
				Suit le numéro du run pour la méthodes testée.
				Puis on retrouve les différentes itérations du warmup avec les valeurs demandées du benchmark pour l'itération donnée.
				De la même façon suivent les itérations de test.
			
			\subsection{Résultats}
				Les résultats nous affichent (dans notre cas) le nombre d'opérations moyen par seconde avec un taux de fiabilité.
				De plus les valeurs min et max sont aussi présentes avec leur ecart-type.
				
			\subsection{Analyse}
				Problème pour l'analyse car on a benchmark que la factorielle non récursive.
				Cependant la semaine suivante JMH se lançait avec un JDK9.
				Afin de ne pas falsifier les résultats nous sommes passés au MyBenchmark2.
				
				De plus la factorielle fait rapidement exploser les int et ne nous permet donc pas de tester avec des paramètres d'entré très différents.
		
		\section{Résultats - MyBenchmark2}
			(Code disponible en \autoref{sec:mybenchmark2})
			
			\subsection{Analyse}
				Première chose que l'on remarque est que pour 50000 éléments la méthode de concaténation de string affiche un score de 0 ops/s.
				On peut se douter que cela n'est pas normal.
				Cela pourrait être du au fait que le nombre réel est tellement faible qu'une fois arrondi il devient 0.
				
				Ensuite on voit que de manière générale fusionner 50 morceaux de chaine est plus rapide que en fusionner 50000.
				
				Si l'on compare les méthodes testées:
				\begin{easylist}
					@ Sur des petites fusions (50 éléments) le StringBuilder domine largement le StringBuffer lui même largement supérieur à la concaténation simple.
					@ Sur des grandes fusions (50000 éléments) StringBuffer et StringBuilder sont equivalents pendant que la concaténation est en PLS.
				\end{easylist}
				
				Les tests ont été refaits avec une capacité initiale de 500 pour les StringBuffer et StringBuilder.
				On remarque que sur des petites itérations, la performance du StringBuffer et StringBuilder est dégradée.
				Cela s'explique par le fait qu'une allocation de 500 est inutile pour 50 éléments.
				
				En revanche pour des fusions de 50000 éléments les performances sont:
				\begin{easylist}
					@ Légèrement améliorées pour le StringBuilder
					@ Très largement améliorées pour le StringBuffer
				\end{easylist}
				
				De son coté la concaténation simple pleure toujours.
				
\appendix
\chapter{MyBenchmark\label{sec:mybenchmark}}
	\lstinputlisting[caption=MyBenchmark.java, language=JAVA]{jmh/src/main/java/fr/polytechtours/javaperformance/tp2/MyBenchmark.java}
	
\chapter{MyBenchmark\label{sec:mybenchmark2}}
	\lstinputlisting[caption=MyBenchmark2.java, language=JAVA]{jmh/src/main/java/fr/polytechtours/javaperformance/tp2/MyBenchmark2.java}
	
	\lstinputlisting[caption=out2.log]{out2.log}
	
	\lstinputlisting[caption=out2.log avec capacité initiale]{out3.log}
			
\end{document}